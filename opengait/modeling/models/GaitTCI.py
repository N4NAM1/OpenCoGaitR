import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPModel
from ..base_model import BaseModel

class TGDS(nn.Module):
    """
    ğŸ”¥ åˆ›æ–°æ¨¡å—ï¼šText-Guided Dynamic Sampling (æ–‡æœ¬å¼•å¯¼çš„æ—¶åºåŠ¨æ€é‡‡æ ·èšåˆ)
    åŠŸèƒ½ï¼šåˆ©ç”¨æ–‡æœ¬æŒ‡ä»¤ä½œä¸º Queryï¼Œä»è§†é¢‘å¸§åºåˆ—ä¸­åŠ¨æ€é‡‡æ ·å¹¶èšåˆæœ€ç›¸å…³çš„æ­¥æ€ç‰¹å¾ã€‚
    """
    def __init__(self, clip_feature_dim: int, projection_dim: int, num_heads: int = 8):
        super(TGDS, self).__init__()

        # ä½¿ç”¨ MultiheadAttention å®ç°æ–‡æœ¬å¼•å¯¼çš„æ—¶åºç­›é€‰
        self.attn = nn.MultiheadAttention(embed_dim=projection_dim, num_heads=num_heads, batch_first=True)

        self.norm = nn.LayerNorm(clip_feature_dim)

    def forward(self, image_seq, text_features):
        """
        Args:
            image_seq: [B, T, D] CLIP è§†é¢‘å¸§ç‰¹å¾åºåˆ—
            text_features: [B, D] CLIP æ–‡æœ¬æŒ‡ä»¤ç‰¹å¾
        Returns:
            aggregated_feat: [B, D] èšåˆåçš„æ—¶åºç‰¹å¾
        """
        # 1. æŠ•å½±åˆ°æ³¨æ„åŠ›ç©ºé—´
        # Query æ¥è‡ªæ–‡æœ¬ (B, 1, Proj_D)ï¼ŒKey/Value æ¥è‡ªå›¾åƒåºåˆ— (B, T, Proj_D)
        query = text_features.unsqueeze(1)  # [B, 1, D]
        key = image_seq                     # [B, T, D]
        value = image_seq                   # [B, T, D]

        # 2. ğŸ”¥ è·¨æ¨¡æ€æ—¶åºæ³¨æ„åŠ›è®¡ç®—
        # attn_output æ•æ‰äº†ä¸æ–‡æœ¬æŒ‡ä»¤æœ€åŒ¹é…çš„å¸§ç»„åˆ
        attn_output, _ = self.attn(query, key, value)
        
        # 3. Add&Normï¼ˆä½¿ç”¨å‡å€¼ä½œä¸º Baseï¼‰
        res = attn_output.squeeze(1)
        output = self.norm(res + image_seq.mean(dim=1))
        
        return output

class Combiner(nn.Module):
    """ ä¿æŒåŸæœ‰ä¼˜ç§€çš„é—¨æ§èåˆé€»è¾‘ """
    def __init__(self, clip_feature_dim: int, projection_dim: int, hidden_dim: int):
        super(Combiner, self).__init__()
        self.text_projection_layer = nn.Linear(clip_feature_dim, projection_dim)
        self.image_projection_layer = nn.Linear(clip_feature_dim, projection_dim)
        self.dropout1 = nn.Dropout(0.5)
        self.dropout2 = nn.Dropout(0.5)
        self.combiner_layer = nn.Linear(projection_dim * 2, hidden_dim)
        self.output_layer = nn.Linear(hidden_dim, clip_feature_dim) 
        self.dropout3 = nn.Dropout(0.5)
        self.dynamic_scalar = nn.Sequential(
            nn.Linear(projection_dim * 2, hidden_dim), 
            nn.ReLU(), nn.Dropout(0.5), nn.Linear(hidden_dim, 1), nn.Sigmoid()
        )

    def forward(self, image_features, text_features):
        text_projected = self.dropout1(F.relu(self.text_projection_layer(text_features)))
        image_projected = self.dropout2(F.relu(self.image_projection_layer(image_features)))
        raw_combined = torch.cat((text_projected, image_projected), -1)
        combined_features = self.dropout3(F.relu(self.combiner_layer(raw_combined)))
        mlp_out = self.output_layer(combined_features)
        sigma = self.dynamic_scalar(raw_combined)
        output = mlp_out + sigma * text_features + (1 - sigma) * image_features
        return F.normalize(output, p=2, dim=-1, eps=1e-6)

class GaitTCI(BaseModel):
    def __init__(self, cfgs, training=True):
        super().__init__(cfgs, training)
        
    def build_network(self, model_cfg):
        model_id = model_cfg.get('backbone', "openai/clip-vit-base-patch32")
        print(f"ğŸ—ï¸ Building Gait-TCI Model: {model_id}")
        
        self.clip = CLIPModel.from_pretrained(model_id)
        for param in self.clip.parameters():
            param.requires_grad = False
            
        self.feature_dim = self.clip.projection_dim 
        proj_dim = model_cfg.get('projection_dim', 512)
        hidden_dim = model_cfg.get('hidden_dim', 2048)
        
        # ğŸ”¥ å®ä¾‹åŒ–ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—
        self.tgds = TGDS(self.feature_dim, proj_dim)
        self.combiner = Combiner(self.feature_dim, proj_dim, hidden_dim)
        
        self.logit_scale = nn.Parameter(torch.ones([]) * 2.6592)
        
        if self.training:
            loss_cfg = self.cfgs.get('loss_cfg', {})
            self.loss_alpha = loss_cfg.get('alpha', 0.5)
            self.loss_fn = nn.CrossEntropyLoss()

    def _encode_image(self, img_tensor):
        """ ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šä¸å†åœ¨å†…éƒ¨åš Max Poolingï¼Œä¿ç•™åºåˆ—ä¿¡æ¯ """
        B, T, C, H, W = img_tensor.shape
        img_flat = img_tensor.view(-1, C, H, W)
        with torch.no_grad():
            feat = self.clip.get_image_features(img_flat)
        # è¿”å› [B, T, D]
        feat = feat.view(B, T, -1)
        return F.normalize(feat, p=2, dim=-1, eps=1e-6)

    def _encode_text(self, input_ids, attention_mask):
        with torch.no_grad():
            feat = self.clip.get_text_features(input_ids, attention_mask)
        return F.normalize(feat, p=2, dim=-1, eps=1e-6)

    def forward(self, inputs):
        if self.training: return self.forward_train(inputs)
        else: return self.forward_test(inputs)
        
    def forward_train(self, inputs):
        ref, tar, txt_ids, txt_mask, inv_ids, inv_mask = inputs
        
        # 1. æå–åºåˆ—ç‰¹å¾
        ref_seq = self._encode_image(ref) # [B, T, D]
        tar_seq = self._encode_image(tar) # [B, T, D]
        txt_feat = self._encode_text(txt_ids, txt_mask) # [B, D]
        inv_feat = self._encode_text(inv_ids, inv_mask) # [B, D]
        
        # 2. ğŸ”¥ ç¬¬ä¸€é˜¶æ®µï¼šæ–‡æœ¬å¼•å¯¼èšåˆ (TGDS)
        # æ­¤æ—¶ ref_feat å˜æˆäº†å— text å¯å‘èšåˆåçš„å•å‘é‡
        ref_agg = self.tgds(ref_seq, txt_feat)
        tar_agg = self.tgds(tar_seq, inv_feat)
        
        # 3. ğŸ”¥ ç¬¬äºŒé˜¶æ®µï¼šCombiner èåˆ
        q_fwd = self.combiner(ref_agg, txt_feat)
        q_inv = self.combiner(tar_agg, inv_feat)
        
        # 4. è®¡ç®— Loss
        logit_scale = self.logit_scale.exp()
        labels = torch.arange(len(q_fwd), device=self.device)
        
        # æ³¨æ„ï¼štar_feat åœ¨å¯¹æ¯”æ—¶ä½¿ç”¨å…¶åºåˆ—çš„å‡å€¼æˆ– Max ä½œä¸º Target è¡¨è¾¾ï¼Œ
        # ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ tar_aggã€‚è¿™é‡Œä¸ºäº†ç¨³å¥ï¼Œä½¿ç”¨ tar_seq çš„ meanã€‚
        tar_target = tar_seq.mean(dim=1)
        logits_fwd = (q_fwd @ tar_target.T) * logit_scale
        loss_fwd = self.loss_fn(logits_fwd, labels)
        
        # é€†å‘ Loss (Cycle Consistency)
        # ç›®æ ‡æ˜¯ q_inv ç»è¿‡å˜æ¢åèƒ½å›åˆ° ref çš„åŸå§‹ç‰¹å¾
        ref_target = ref_seq.mean(dim=1)
        loss_inv = 1.0 - F.cosine_similarity(q_inv, ref_target).mean()
        
        total_loss = loss_fwd + self.loss_alpha * loss_inv
        
        return {'loss': total_loss, 'acc_loss': loss_fwd, 'inv_loss': loss_inv}

    def forward_test(self, inputs):
            ref_list, tar_list, txt_ids, txt_mask, tasks, metas = inputs
            # txt_feat æ˜¯æ•´ä¸ª batch çš„ç‰¹å¾: [B, D] (ä¾‹å¦‚ [4, 512])
            txt_feat = self._encode_text(txt_ids, txt_mask)
            
            ref_feats = []
            tar_feats = []
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ enumerate è·å–ç´¢å¼• iï¼Œä»¥ä¾¿å–å‡ºå¯¹åº”çš„æ–‡æœ¬ç‰¹å¾
            for i, (r, t) in enumerate(zip(ref_list, tar_list)):
                # 1. å‡†å¤‡è§†é¢‘åºåˆ— [1, T, D]
                r = r.unsqueeze(0).to(self.device)
                t = t.unsqueeze(0).to(self.device)
                r_seq = self._encode_image(r) 
                t_seq = self._encode_image(t)
                
                # 2. ğŸ”¥ ä¿®æ­£ï¼šå–å‡ºå½“å‰æ ·æœ¬å¯¹åº”çš„ç¬¬ i ä¸ªæ–‡æœ¬ç‰¹å¾ï¼Œå¹¶ä¿æŒç»´åº¦ [1, D]
                curr_txt_feat = txt_feat[i].unsqueeze(0) 
                
                # 3. ç°åœ¨ Query(Batch=1) å’Œ Key(Batch=1) ç»´åº¦åŒ¹é…äº†
                r_agg = self.tgds(r_seq, curr_txt_feat)
                
                ref_feats.append(r_agg)
                tar_feats.append(t_seq.mean(dim=1)) # Target ç«¯ç»§ç»­ç”¨å‡å€¼
            
            # æ‹¼æ¥å› Batch ç»´åº¦ [B, D]
            ref_feats = torch.cat(ref_feats, dim=0)
            tar_feats = torch.cat(tar_feats, dim=0)
            
            # æœ€ç»ˆ Combiner èåˆ (è¿™é‡Œ txt_feat æ˜¯å®Œæ•´çš„ [B, D]ï¼Œref_feats ä¹Ÿæ˜¯ [B, D]ï¼Œå¯ä»¥ä¸€èµ·è®¡ç®—)
            q_feat = self.combiner(ref_feats, txt_feat)
            
            return {
                "query_feat": q_feat,
                "tar_feat": tar_feats,
                "tasks": tasks,
                "metas": metas
            }